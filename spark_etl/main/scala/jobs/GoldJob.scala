package jobs

import org.apache.spark.sql.{SparkSession, DataFrame, functions => F}
import cats.effect.IO

object GoldJob {

  def run(
    spark: SparkSession,
    silverTables: Map[String, DataFrame]
  ): IO[Unit] = for {
    _ <- IO.println("\nðŸ† Gold Layer: Formatting Final JSON Output...")

    // à¸”à¸¶à¸‡ checkins à¸—à¸µà¹ˆà¸œà¹ˆà¸²à¸™à¸à¸²à¸£ Merge à¹à¸¥à¸° Deduplicate à¸¡à¸²à¹à¸¥à¹‰à¸§à¸ˆà¸²à¸ Silver
    checkinsNormalized = silverTables("checkins_normalized")

    _ <- IO.println("  Aggregating by BIB & Formatting Timestamps...")
    finalDf <- aggregateByBib(spark, checkinsNormalized)

    _ <- IO.println("  Writing final output to JSON...")
    _ <- writeGoldJSON(finalDf)

    _ <- IO.println("âœ… Gold Complete\n")
  } yield ()

  private def aggregateByBib(spark: SparkSession, merged: DataFrame): IO[DataFrame] = IO {
    import spark.implicits._

    merged
      // à¸à¸£à¸­à¸‡à¹€à¸‰à¸žà¸²à¸°à¸—à¸µà¹ˆà¸¡à¸µà¹€à¸§à¸¥à¸²à¸ªà¹à¸à¸™ (à¹€à¸œà¸·à¹ˆà¸­à¸à¸£à¸“à¸µ Runner à¸—à¸µà¹ˆà¸¢à¸±à¸‡à¹„à¸¡à¹ˆà¹„à¸”à¹‰à¸§à¸´à¹ˆà¸‡)
      .filter($"scannedAt".isNotNull)
      // à¹à¸›à¸¥à¸‡à¸ˆà¸²à¸ UTC (Silver) à¹€à¸›à¹‡à¸™ Bangkok (Local) à¹à¸¥à¸°à¸—à¸³ ISO format
      .withColumn("checkpointAt",
        F.date_format(F.from_utc_timestamp($"scannedAt", "Asia/Bangkok"), "yyyy-MM-dd'T'HH:mm:ssXXX")
      )
      // à¹€à¸£à¸µà¸¢à¸‡à¸¥à¸³à¸”à¸±à¸šà¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸«à¹‰ checkpoints à¹ƒà¸™ List à¹€à¸£à¸µà¸¢à¸‡à¸•à¸²à¸¡à¸„à¸§à¸²à¸¡à¸ˆà¸£à¸´à¸‡
      .orderBy($"bibNumber", $"sequenceOrder")
      .groupBy($"bibNumber".alias("bib"))
      .agg(
        F.collect_list(
          F.struct(
            $"checkpoint_id".alias("checkpointId"),
            $"checkpointAt"
          )
        ).alias("checkpoints")
      )
  }

  private def writeGoldJSON(df: DataFrame): IO[Unit] = IO {
    val outputPath = "s3a://tatar-race-data/gold/final_checkins_25km"

    df.coalesce(1)
      .write
      .mode("overwrite")
      .json(outputPath)

    println(s"âœ… Gold Data written to Private S3: $outputPath")
  }
}
